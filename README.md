# UROP

### Topic

- NLP
- Knowledge Distillation
- Pruning

### Reference Site

- [ACL](https://aclanthology.org/events/acl-2023/)
- [NAACL](https://aclanthology.org/events/naacl-2022/)
- [EMNLP](https://aclanthology.org/events/emnlp-2022/)


<br>

## üèÉüèª‚Äç‚ôÄÔ∏è Schedule
- Ïó∞Íµ¨Í≥ÑÌöçÏÑú Ï†úÏ∂ú : 23-09-15 
- Ïó∞Íµ¨Î≥¥Í≥†ÏÑú Ï†úÏ∂ú : 23-12-15

<br>

## üìò Document
<br>

### Knowledge Distillation

| Summary | Title & Authors | Introduction | Links | 
| :----: | :----: | :---: | :---: |
| [#1 LGTM](https://velog.io/@yun_haaaa/%EB%85%BC%EB%AC%B8-%EC%9D%BD%EA%B8%B0-1-LGTM) |[![Publish](https://img.shields.io/badge/Conference-ACL'23%20-blue)]()<br>[Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation](https://arxiv.org/abs/2305.09651) <br> Yuxin Ren, Zihan Zhong, Xingjian Shi, Yi Zhu, Chun Yuan, Mu Li | ![LGTM](https://github.com/YunHaaaa/UROP/assets/63325450/730a2e9e-606c-483e-92b7-4270bce1c00d) |[Github](https://github.com/twinkle0331/LGTM) <br> [Paper](https://arxiv.org/abs/2305.09651) |
| [#2 SCOTT](https://velog.io/@yun_haaaa/%EB%85%BC%EB%AC%B8-%EC%9D%BD%EA%B8%B0-2-SCOTT-Self-Consistent-Chain-of-Thought-Distillation)|[![Publish](https://img.shields.io/badge/Conference-ACL'23%20Outstanding-blue)]()<br>[SCOTT: Self-Consistent Chain-of-Thought Distillation](https://arxiv.org/abs/2305.01879) <br> Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang Ren | ![SCOTT](https://github.com/YunHaaaa/UROP/assets/63325450/5160575e-2eac-44c9-a697-0d123e96ee30) |[Paper](https://arxiv.org/abs/2305.01879) |
| []() |[![Publish](https://img.shields.io/badge/Conference-ICML'23-blue)]()<br>[Specializing Smaller Language Models towards Multi-Step Reasoning](https://arxiv.org/abs/2301.12726) <br> Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot | ![ModelSpecialization](https://github.com/YunHaaaa/UROP/assets/63325450/3974db2a-448a-4f73-9968-3c40ee145dd3) |[Github](https://github.com/FranxYao/FlanT5-CoT-Specialization) <br> [Paper](https://arxiv.org/abs/2301.12726)|

<br>

### Pruning

| Summary | Title & Authors | Introduction | Links | 
| :----: | :----: | :---: | :---: |
| []() |[![Publish](https://img.shields.io/badge/Conference-ACL'23%20-blue)]()<br>[PuMer: Pruning and Merging Tokens for Efficient Vision Language Models](https://aclanthology.org/2023.acl-long.721/) <br> Qingqing Cao, Bhargavi Paranjape, and Hannaneh Hajishirzi | ![PuMer](https://github.com/YunHaaaa/UROP/assets/63325450/43896cd9-9693-4d08-b849-37ebd590d93e)|[Github](https://github.com/csarron/PuMer) <br> [Paper](https://aclanthology.org/2023.acl-long.721/) |


<br>

## üëäüèª Troubleshooting


<br>

## ‚úèÔ∏è Report


<br>

## üí¨ Meeting


<br>
