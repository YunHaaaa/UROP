# UROP 

### Topic

- NLP
- Knowledge Distillation
- Pruning

### Reference Site

- [ACL](https://aclanthology.org/events/acl-2023/)
- [NAACL](https://aclanthology.org/events/naacl-2022/)
- [EMNLP](https://aclanthology.org/events/emnlp-2022/)


<br>

## üèÉüèª‚Äç‚ôÄÔ∏è Schedule
- Ïó∞Íµ¨Í≥ÑÌöçÏÑú Ï†úÏ∂ú : 23-09-15 
- Ïó∞Íµ¨Î≥¥Í≥†ÏÑú Ï†úÏ∂ú : 23-12-15

<br>

## üìò Document
<br>

### Knowledge Distillation

| Summary | Title & Authors | Introduction | Links | 
| :----: | :----: | :---: | :---: |
| [#1 LGTM](https://velog.io/@yun_haaaa/%EB%85%BC%EB%AC%B8-%EC%9D%BD%EA%B8%B0-1-LGTM) |[![Publish](https://img.shields.io/badge/Conference-ACL'23%20-blue)]()<br>[Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation](https://arxiv.org/abs/2305.09651) <br> Yuxin Ren, Zihan Zhong, Xingjian Shi, Yi Zhu, Chun Yuan, Mu Li | <img src="https://github.com/YunHaaaa/UROP/assets/63325450/730a2e9e-606c-483e-92b7-4270bce1c00d" alt="LGTM" width="350"> |[Github](https://github.com/twinkle0331/LGTM) <br> [Paper](https://arxiv.org/abs/2305.09651) |
| [#2 SCOTT](https://velog.io/@yun_haaaa/%EB%85%BC%EB%AC%B8-%EC%9D%BD%EA%B8%B0-2-SCOTT-Self-Consistent-Chain-of-Thought-Distillation)|[![Publish](https://img.shields.io/badge/Conference-ACL'23%20Outstanding-blue)]()<br>[SCOTT: Self-Consistent Chain-of-Thought Distillation](https://arxiv.org/abs/2305.01879) <br> Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang Ren | ![SCOTT](https://github.com/YunHaaaa/UROP/assets/63325450/5160575e-2eac-44c9-a697-0d123e96ee30) |[Paper](https://arxiv.org/abs/2305.01879) |
| [#3 KD-QAT](https://velog.io/@yun_haaaa/%EB%85%BC%EB%AC%B8%EC%9D%BD%EA%B8%B0-6-Understanding-and-Improving-Knowledge-Distillation-for-Quantization-Aware-Training-of-Large-Transformer-Encoders) |[![Publish](https://img.shields.io/badge/Conference-EMNLP'22%20-blue)]()<br>[Understanding and Improving Knowledge Distillation for Quantization Aware Training of Large Transformer Encoders](https://aclanthology.org/2022.emnlp-main.450/) <br> Minsoo Kim, Sihwa Lee, Suk-Jin Hong, Du-Seong Chang, and Jungwook Choi | <img src="https://github.com/YunHaaaa/UROP/assets/63325450/283d971d-2146-447d-8c1d-5ba32221f5f0" alt="Symbolic KD" width="350"> |[Github](https://github.com/MarsJacobs/kd-qat-large-enc) <br> [Paper](https://aclanthology.org/2022.emnlp-main.450/) |
| [#4 FlanT5](https://velog.io/@yun_haaaa/%EB%85%BC%EB%AC%B8-%EC%9D%BD%EA%B8%B0-7-Specializing-Smaller-Language-Models-towards-Multi-Step-ReasoningEncoders) |[![Publish](https://img.shields.io/badge/Conference-ICML'23-blue)]()<br>[Specializing Smaller Language Models towards Multi-Step Reasoning](https://arxiv.org/abs/2301.12726) <br> Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot | ![ModelSpecialization](https://github.com/YunHaaaa/UROP/assets/63325450/3974db2a-448a-4f73-9968-3c40ee145dd3) |[Github](https://github.com/FranxYao/FlanT5-CoT-Specialization) <br> [Paper](https://arxiv.org/abs/2301.12726)|
| [#5 DML](https://velog.io/@yun_haaaa/%EB%85%BC%EB%AC%B8-%EC%9D%BD%EA%B8%B0-8-Deep-Mutual-Learning) | <br>[Deep Mutual Learning](https://arxiv.org/abs/1706.00384) <br> Ying Zhang, Tao Xiang, Timothy M. Hospedales, Huchuan Lu | ![image](https://github.com/YunHaaaa/UROP/assets/63325450/5e1e7820-af95-4eb6-a93e-b7e3611317e1) | [Paper](https://arxiv.org/abs/1706.00384) |


<br>

### Pruning

| Summary | Title & Authors | Introduction | Links | 
| :----: | :----: | :---: | :---: |
| [#1 PuMer](https://velog.io/@yun_haaaa/%EB%85%BC%EB%AC%B8-%EC%9D%BD%EA%B8%B0-3-PuMer-Pruning-and-Merging-Tokens-for-Efficient-Vision-Language-Models)<br><br>[Evaluation](https://velog.io/@yun_haaaa/%EB%85%BC%EB%AC%B8-%EC%9D%BD%EA%B8%B0-4-PuMer-Pruning-and-Merging-Tokens-for-Efficient-Vision-Language-Models) |[![Publish](https://img.shields.io/badge/Conference-ACL'23%20-blue)]()<br>[PuMer: Pruning and Merging Tokens for Efficient Vision Language Models](https://aclanthology.org/2023.acl-long.721/) <br> Qingqing Cao, Bhargavi Paranjape, and Hannaneh Hajishirzi | ![PuMer](https://github.com/YunHaaaa/UROP/assets/63325450/43896cd9-9693-4d08-b849-37ebd590d93e) |[Github](https://github.com/csarron/PuMer) <br> [Paper](https://aclanthology.org/2023.acl-long.721/) |
| [#2 PLMS](https://velog.io/@yun_haaaa/%EB%85%BC%EB%AC%B8-%EC%9D%BD%EA%B8%B0-5-Specializing-Pre-trained-Language-Models-for-Better-Relational-Reasoning-via-Network-Pruning) |[![Publish](https://img.shields.io/badge/Conference-NAACL'22%20-blue)]()<br>[Specializing Pre-trained Language Models for Better Relational Reasoning via Network Pruning](https://aclanthology.org/2022.findings-naacl.169/) <br> Siyu Ren and Kenny Zhu | <img src="https://github.com/YunHaaaa/UROP/assets/63325450/231b166c-a0f2-4307-975a-fa5c56f03c08" alt="Symbolic KD" width="350"> |[Github](https://github.com/DRSY/LAMP) <br> [Paper](https://aclanthology.org/2022.findings-naacl.169/) |
| [üë©üèª‚Äçüíªüìñ]() | <br>[Dynamic Model Pruning with Feedback](https://arxiv.org/abs/2006.07253) <br> Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, Martin Jaggi | ![image](https://github.com/YunHaaaa/UROP/assets/63325450/eafc9583-9d6e-4b82-a33a-6629ef663f8d) | [Paper](https://arxiv.org/abs/2006.07253) |
| [üë©üèª‚Äçüíªüìñ]() |[![Publish](https://img.shields.io/badge/Conference-NAACL'22%20-blue)]()<br>[Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning](https://aclanthology.org/2022.gebnlp-1.6/) <br> Przemyslaw Joniak and Akiko Aizawa | <img src="https://github.com/YunHaaaa/UROP/assets/63325450/05f805ba-3cb4-43bf-beab-0097e0614ce7" alt="Symbolic KD" width="350"> |[Github](https://github.com/kainoj/pruning-bias) <br> [Paper](https://aclanthology.org/2022.gebnlp-1.6/) |



<br>

## üëäüèª Troubleshooting


<br>

## ‚úèÔ∏è Report


<br>

## üí¨ Meeting


<br>
