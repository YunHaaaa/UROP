# UROP
- NLP
- Knowledge Distillation

<br>

## 🏃🏻‍♀️ Schedule
- 연구계획서 제출 : 23-09-15 
- 연구보고서 제출 : 23-12-15

<br>

## 📘 Document



| Summary | Title & Authors | Introduction | Links | 
|:----|  :----: | :---:| :---:|
| [#1 LGTM](https://velog.io/@yun_haaaa/%EB%85%BC%EB%AC%B8-%EC%9D%BD%EA%B8%B0-1-LGTM) |[![Publish](https://img.shields.io/badge/Conference-ACL'23%20-blue)]()<br>[Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation](https://arxiv.org/abs/2305.09651) <br> Yuxin Ren, Zihan Zhong, Xingjian Shi, Yi Zhu, Chun Yuan, Mu Li |<img width="1002" alt="image" src="figures/LGTM.png"> |[Github](https://github.com/twinkle0331/LGTM) <br> [Paper](https://arxiv.org/abs/2305.09651) |
| [#2 SCOTT](https://velog.io/@yun_haaaa/%EB%85%BC%EB%AC%B8-%EC%9D%BD%EA%B8%B0-2-SCOTT-Self-Consistent-Chain-of-Thought-Distillation)|[![Publish](https://img.shields.io/badge/Conference-ACL'23%20Outstanding-blue)]()<br>[SCOTT: Self-Consistent Chain-of-Thought Distillation](https://arxiv.org/abs/2305.01879) <br> Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang Ren |<img width="1002" alt="image" src="figures/SCOTT.png"> |[Paper](https://arxiv.org/abs/2305.01879) |

<br>

## 👊🏻 Troubleshooting


<br>

## ✏️ Report


<br>

## 💬 Meeting


<br>
